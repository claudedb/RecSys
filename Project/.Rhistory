1+1
help(qunif)
qunif(1)
qunif(1=0.75)
qunif(0.75)
install.packages('xgboost')
source('~/Polymtl/LOG6308/clustering.R', echo=TRUE)
dim(dist_eucl)
clusters<-hclust(dist_eucl,method='complete')
View(dist_eucl)
dist_eucl<-sqrt(sweep(sweep(-2*m %*% t(m),1,sums_2,"+"),2,sums_2,"+"))
clusters<-hclust(dist_eucl,method='complete')
clusters<-hclust(as.dist(dist_eucl),method='complete')
clusters
try<-cutree(clusters, k = 10)
try
clusters<-hclust(as.dist(dist_eucl),method='centroid')
try<-cutree(clusters, k = 10)
try
clusters<-hclust(as.dist(dist_eucl))
try<-cutree(clusters, k = 10)
try
dist_eucl<-matrix.cos(m)
matrix.cos <- function(m) {
(m %*% t(m)) / ( t(matrix(sqrt(rowSums(m^2)),nrow(m),nrow(m)))
* (sqrt(rowSums(m^2))) )
}
sums_2<-rowSums(m^2)
dist_eucl<-matrix.cos(m)
clusters<-hclust(as.dist(dist_eucl))
try<-cutree(clusters, k = 10)
try
avg.user<-rowSums(m)/rowSums(m>0)
m.class<-cbind(calc.clusters,m)
calc.clusters<-cutree(clusters, k = 10)
m.class<-cbind(calc.clusters,m)
View(m.class)
clusters.avg<-aggregate(.~calc.clusters,m.class,mean())
clusters.avg<-aggregate(.~calc.clusters,m.class,'mean')
m.class<-as.dataframe(cbind(calc.clusters,m))
m.class<-as.data.frame(cbind(calc.clusters,m))
clusters.avg<-aggregate(.~calc.clusters,m.class,'mean')
View(clusters.avg)
clusters.avg<-aggregate(.~calc.clusters,m.class,'sum')/aggregate(.~calc.clusters,m.class>0,'sum')
View(m.class)
m.class[m.class==0]<-NA
clusters.avg<-aggregate(.~calc.clusters,m.class,'mean')
mean.nona<-function(m){
return(mean(m,na.rm=T))
}
clusters.avg<-aggregate(.~calc.clusters,m.class,FUN='mean.nona')
clusters.avg<-aggregate(.~calc.clusters,m.class,FUN='mean',na.action=na.omit)
calc.clusters<-cutree(clusters, k = 10)
m.class<-as.data.frame(cbind(calc.clusters,m))
m.class[m.class==0]<-NA
mean.nona<-function(m){
return(mean(m,na.rm=T))
}
clusters.avg<-aggregate(.~calc.clusters,m.class,FUN='mean',na.action=na.omit)
m.class<-as.data.frame(cbind(calc.clusters,m))
clusters.avg<-aggregate(.~calc.clusters,m.class,FUN='mean',na.action=na.omit)
m.class[m.class==0]<-NA
View(m.class)
clusters.avg<-aggregate(.~calc.clusters,m.class,FUN='mean',na.action=na.omit)
clusters.avg<-aggregate(.~calc.clusters,m.class,FUN='mean',na.rm=T,na.action=NULL)
View(clusters.avg)
is.nan(clusters.avg)<-NA
is.nan(clusters.avg)
source('~/GitHub/LOG6308/TP 2/TP2-V1.R', echo=TRUE)
which(m[doc.id.q1,]==1)
s
S
SS<-colSums(which(m[doc.id.q1,]==1))
SS<-colSums(m[which(m[doc.id.q1,]==1),])
ss
SS
SS<-colSums(m[which(m[doc.id.q1,]==1),])+m[,doc.id.q1]
SS<-colSums(m[which(m[doc.id.q1,]==1),])+m[doc.id.q1,]
SS[SS>=1]<-1
ref.ref.PR<-S.prime*PageRank
S.prime<-colSums(m[which(m[doc.id.q1,]==1),])+m[doc.id.q1,]
S.prime[S.prime>=1]<-1
ref.ref.PR<-S.prime*PageRank
m[doc.id.q1,]
RR.PageRank<-S.prime*PageRank
RR.PageRank<-S.prime*PageRank
recommendations.RR<-RR.PageRank[order(-recommendations.RR)][1:10]
recommendations.RR<-RR.PageRank[order(-RR.PageRank)][1:10]
colnames(recommendations.RR)
colnames(recommendations.references)
S
references.PageRank<-m[doc.id.q1,]*PageRank
recommendations.references<-references.PageRank[order(-references.PageRank)][1:10]
colnames(recommendations.references)
S.prime<-colSums(m[which(m[doc.id.q1,]==1),])+m[doc.id.q1,]
S.prime[S.prime>=1]<-1
RR.PageRank<-S.prime*PageRank
recommendations.RR<-RR.PageRank[order(-RR.PageRank)][1:10]
colnames(recommendations.RR)
colnames(recommendations.references)
vecteur.q2<-m[,doc.id.q1]
vecteur.q2<-m[,"x"+doc.id.q1]
doc.id.q2<-"X"+doc.id.q1
doc.id.q2<-paste("X",doc.id.q1)
doc.id.q2<-paste("X",doc.id.q1,sep="")
vecteur.q2<-m[,doc.id.q2]
positions.votes.communs=(m[,doc.id.q2]*m)>0
positions.votes.communs<-(m[,doc.id.q2]*m)>0
votes.communs<-(colSums((m[,doc.id.q2]*m)>0,na.rm=T))
doc.id.q2<-paste("X",doc.id.q1,sep="")
vecteur.q2<-m[,doc.id.q2]
#On identifie d'abord avec une matrice logique les positions des votes communs avec l'article actuel
positions.votes.communs<-(m[,doc.id.q2]*m)>0
#votes.communs est un vecteur de longueur 1090 où chaque instance représente le nombre de votes communs avec l'article actuel.
votes.communs<-(colSums((m[,doc.id.q2]*m)>0,na.rm=T))
k<-vecteur.q2%*%m
m.clean<-m
m.clean[!positions.votes.communs]<-NA
vecteur.q2.clean<-vecteur.q2
vecteur.q2.clean<-matrix(vecteur.q2.clean,nrow=dim(m)[1],ncol=dim(m)[2])
vecteur.q2.clean<-as(vecteur.q2.clean, "dgCMatrix")
vecteur.q2.clean[!positions.votes.communs]=NA
n=sqrt(colSums(m.clean^2,na.rm=T))
d=sqrt(colSums(vecteur.q2.clean^2,na.rm=T))
cosinus.q2<-k/(n*d)
cosinus.q2<-as.vector(cosinus.q2)
cosinus.q2[is.nan(cosinus.q2)]<-0
#Paramètre à modifier parce que si il n'est pas là, tous les articles avec les plus hauts cosinus sont ceux
#qui n'ont pas beaucoup de votes communs et que les votes sont très similaires
cosinus.q2.VC<-cosinus.q2
#cosinus.q2.VC[votes.communs<=15]<-NA
distance.ii<-sqrt(colSums((m.clean-vecteur.q3)^2,na.rm=T))/votes.communs
distance.ii[votes.communs<5]<-NA
#On sélectionne les 20 items les plus proches et
distance.top20.ii<-min.nindex(distance.ii,21)
distance.top20.ii<-distance.top20.ii[distance.top20.ii!=q2.id]
cosinus.q3=cosinus.q2[distance.top20.ii]
#On apporte une pondération fonction du nombre de votes communs avec Star trek
cosinus.q2.VC=(pmax(votes.communs[distance.top20.ii],5)/5)*cosinus.q2[distance.top20.ii]
facteur.k<-1/sum(abs(cosinus.q2.VC),na.rm=T)
avg.article<-sum(m[,doc.id.q2])/sum(m[,doc.id.q2]>0)
#m.estim représente la matrice m qu'on a filtré avec les 20 plus proches voisins, elle est donc de dimension 1090x20.
m.estim<-m
m.estim[m.estim==0]<-NA
m.estim<-m.estim[,distance.top20.ii]
#Calcul de l'Estimation du vote basé sur les plus proches voisins
ecart=t(t(m.estim)-colMeans(m.estim,na.rm=T))
ecart.mat=as.matrix(ecart)
terme=t(t(ecart.mat)*cosinus.q2.VC)
#On prédit même pour ceux ayant déjà un vote pour l'article
estimation=avg.startrek+facteur.k*rowSums(terme,na.rm=T)
#On donne une valeur manquante aux users qui n'ont pas de votes communs avec les 20 plus proches voisins du film actuel
estimation[which(rowSums(m.estim,na.rm=T)==0)]=NA
doc.id.q2<-paste("X",doc.id.q1,sep="")
vecteur.q2<-m[,doc.id.q2]
positions.votes.communs<-(m[,doc.id.q2]*m)>0
votes.communs<-(colSums((m[,doc.id.q2]*m)>0,na.rm=T))
k<-vecteur.q2%*%m
k<-vecteur.q2%*%as.matrix(m)
m.clean<-as.matrix(m)
m.clean[!positions.votes.communs]<-NA
vecteur.q2.clean<-vecteur.q2
vecteur.q2.clean<-matrix(vecteur.q2.clean,nrow=dim(m)[1],ncol=dim(m)[2])
vecteur.q2.clean<-as(vecteur.q2.clean, "dgCMatrix")
vecteur.q2.clean[!positions.votes.communs]=NA
n=sqrt(colSums(m.clean^2,na.rm=T))
d=sqrt(colSums(vecteur.q2.clean^2,na.rm=T))
cosinus.q2<-k/(n*d)
cosinus.q2<-as.vector(cosinus.q2)
cosinus.q2[is.nan(cosinus.q2)]<-0
cosinus.q2.VC<-cosinus.q2
distance.ii<-sqrt(colSums((m.clean-vecteur.q3)^2,na.rm=T))/votes.communs
distance.ii<-sqrt(colSums((m.clean-vecteur.q2)^2,na.rm=T))/votes.communs
distance.ii[votes.communs<5]<-NA
distance.top20.ii<-min.nindex(distance.ii,21)
distance.top20.ii<-distance.top20.ii[distance.top20.ii!=q2.id]
cosinus.q2<-cosinus.q2[distance.top20.ii]
cosinus.q2.VC=(pmax(votes.communs[distance.top20.ii],5)/5)*cosinus.q2[distance.top20.ii]
facteur.k<-1/sum(abs(cosinus.q2.VC),na.rm=T)
avg.article<-sum(m[,doc.id.q2])/sum(m[,doc.id.q2]>0)
m.estim<-as.matrix(m)
m.estim[m.estim==0]<-NA
m.estim<-m.estim[,distance.top20.ii]
ecart=t(t(m.estim)-colMeans(m.estim,na.rm=T))
ecart.mat=as.matrix(ecart)
terme=t(t(ecart.mat)*cosinus.q2.VC)
estimation=avg.article+facteur.k*rowSums(terme,na.rm=T)
estimation[which(rowSums(m.estim,na.rm=T)==0)]=NA
estimation
cosinus.q2
cosinus.q2<-cosinus.vm(vecteur.q2,m)
cosinus.vm <- function(v,m) {
n <- sqrt(colSums(m^2)); (v %*% m)/(n * sqrt(sum(v^2)))
return(n)
}
cosinus.q2<-cosinus.vm(vecteur.q2,m)
cosinus.q2<-cosinus.vm(vecteur.q2,as.matrix(m))
recommendations.ii<-cosinus.q2[order(-cosinus.q2)][1:10]
colnames(cosinus.q2)
cosinus.q2
colnames(recommendations.ii)
recommendations.ii<-cosinus.q2[order(-cosinus.q2)][1:10]
rownames(recommendations.ii)
recommendations.ii
recommendations.ii[1,]
colnames(recommendations.ii)
colnames(data.frame(recommendations.ii))
data.frame(recommendations.ii)
rownames(data.frame(recommendations.ii))
colnames(recommendations.RR)
colnames(recommendations.references)
source('~/GitHub/RecSys/Project/ProjetCode.R', echo=TRUE)
m.poly<-m.text[which(courses.data$university=="Poly")]
m.poly<-m.text[which(courses.data$university=="Poly"),]
dim(m.poly)
essai<- lsa(m.poly,dims=dimcalc_share())
essai
colnames(m.text) <- terms.data
terms.data
colnames(m.text) <- as.vector(terms.data)
dim(terms.data)
colnames(m.text) <- as.vector(terms.data[,1])
m.text
rownames(m.text) <- as.vector(courses.data[,2])
colnames(m.text) <- as.vector(terms.data[,1])
m.text
source('~/GitHub/RecSys/Project/ProjetCode.R', echo=TRUE)
essai$sk
essai$tk
dim(essai$tk)
dim(essai$dk)
dim(essai$tk)
m.eval<-essai$tk %*% essai$sk %*% t(essai$dk)
dim(essai$sk)
diag(essai$sk)
m.eval<-essai$tk %*% diag(essai$sk) %*% t(essai$dk)
dim(m.eval)
sum(abs(m-m.eval))
sum(abs(m.poly-m.eval))
m.eval
m.eval[1,1]
m.eval[2,1]
m.eval[6,1]
m.eval[36,1]
source('~/GitHub/RecSys/Project/ProjetCode.R', echo=TRUE)
lsa.elapsedTime
lsaSpace
source('~/GitHub/RecSys/Project/ProjetCode.R', echo=TRUE)
source('~/GitHub/RecSys/Project/ProjetCode.R', echo=TRUE)
source('~/GitHub/RecSys/Project/ProjetCode.R', echo=TRUE)
View(text.data)
text.data<-read.table("data/out.matrix",sep=" ",skip=2)
m.text <- sparseMatrix(text.data[,2],text.data[,1],x=text.data[,3])
courses.data<-read.table('data/out.docs',sep="/")
colnames(courses.data)<-c('university','code')
terms.data<-read.table('data/out.terms')
colnames(m.text) <- as.vector(courses.data[,2])
rownames(m.text) <- as.vector(terms.data[,1])
m.poly<-m.text[,which(courses.data$university=="Poly")]
View(text.data)
colnames(m.text)<-c('courses','terms','count')
colnames(text.data)<-c('courses','terms','count')
View(text.data)
id.poly <- which(courses.data$university=="Poly")
id.hec <- which(courses.data$universtity=="hec")
View(courses.data)
View(text.data)
term.count <- text.data[text.data$courses == id.poly ] %>% group_by(terms) %>% summarise(total=sum(count))
term.count <- text.data[ text.data$courses %in% id.poly ] %>% group_by(terms) %>% summarise(total=sum(count))
text.date[text.data$courses==3]
text.data[text.data$courses==3]
View(text.data)
text.data[text.data$courses==3]
text.data[text.data$courses=='3']
text.data$courses==3
text.data$courses %in% id.poly
text.data<-data.table(read.table("data/out.matrix",sep=" ",skip=2))
m.text <- sparseMatrix(text.data[,2],text.data[,1],x=text.data[,3])
m.text <- sparseMatrix(text.data[,2,],text.data[,1,],x=text.data[,3,])
text.data<-data.table(read.table("data/out.matrix",sep=" ",skip=2))
colnames(text.data)<-c('courses','terms','count')
m.text <- sparseMatrix(text.data$terms,text.data$courses,x=text.data$count)
term.count <- text.data[courses %in% id.poly,(total=sum(n)),by = courses]
term.count <- text.data[courses %in% id.poly,.(total=sum(n)),by = courses]
term.count <- text.data[courses == id.poly,.(total=sum(n)),by = courses]
text.data<-data.table(read.table("data/out.matrix",sep=" ",skip=2))
colnames(text.data)<-c('courses','terms','n')
m.text <- sparseMatrix(text.data$terms,text.data$courses,x=text.data$n)
term.count <- text.data[courses %in% id.poly,.(total=sum(n)),by = courses]
View(term.count)
term.count <- text.data[courses %in% id.poly,.(total=sum(n)),by = terms]
term.count$essai <- terms.data[term.count$terms]
View(terms.data)
term.count$essai <- terms.data[term.count$terms]$V1
term.count$essai <- terms.data[term.count$terms,]$V1
term.count$essai <- terms.data[term.count$terms,]
View(term.count)
source('~/GitHub/RecSys/Project/ProjetCode.R', echo=TRUE)
lsa.elapsedTime
dim(m.text)
tf.idf <- (1 + log(m.text)) * log (ncol(m.text)/colSums(m.text > 0))
ncol(m.text)
colSums(m.text>0)
dim(colSums(m.text>0))
length(colSums(m.text>0))
dim(m.text)
tf.idf <- (1 + log(m.text)) %*% log (ncol(m.text)/(colSums(m.text > 0)+1))
dim(tf.idf)
tf.idf <- (1 + log(m.text)) * log (ncol(m.text)/(colSums(m.text > 0)+1))
length(log (ncol(m.text)/(colSums(m.text > 0)+1)))
dim(log (ncol(m.text)/(colSums(m.text > 0)+1)))
tf.idf <- (1 + log(m.text)) * log (ncol(m.text)/(rowSums(m.text > 0)+1))
dim(tf.idf)
global.entropy <- 1 + rowSums((pij*log2(pij)))/log2(n.courses))
global.entropy <- 1 + rowSums((pij*log2(pij))/log2(n.courses))
pij <- m.text / rowSums(m.text)
global.entropy <- 1 + rowSums((pij*log2(pij))/log2(n.courses))
source('~/GitHub/RecSys/Project/ProjetCode.R', echo=TRUE)
source('~/GitHub/RecSys/Project/ProjetCode.R', echo=TRUE)
source('~/GitHub/RecSys/Project/ProjetCode.R', echo=TRUE)
alt <- (pij * log2(pij))/log2(n.courses)
log2(pij)
as.double(pij)
log2(as.double(pij))
log(pij)
alt <- (pij * log(pij))/log(n.courses)
log(m.text)
source('~/GitHub/RecSys/Project/ProjetCode.R', echo=TRUE)
source('~/GitHub/RecSys/Project/ProjetCode.R', echo=TRUE)
n.courses/(rowSums(m.poly > 0)+1)
log (n.courses/(rowSums(m.poly > 0)+1))
m.poly
rowSums(m.poly)
rowSums(m.poly>0)
rowSums(m.poly>0)+1
n.courses/(rowSums(m.poly > 0)+1))
n.courses/(rowSums(m.poly > 0)+1)
log(m.poly)
sum(log(m.poly)==inf)
sum(is.infinite(log(m.poly)))
m.poly<-m.text[,id.poly]
m.poly<-m.poly[rowSums(m.poly)>0,]
dim(m.poly)
source('~/GitHub/RecSys/Project/ProjetCode.R', echo=TRUE)
source('~/GitHub/RecSys/Project/ProjetCode.R', echo=TRUE)
View(courses.data)
View(term.count)
View(text.data)
m.poly[2:10,40:56]
sum(m.poly)
nrow(m.text)
ncol(m.text)
dim(m.text)
dim(m.poly)
